% ============================================================================
% Bibliography for: Scaling Context Windows to Infinity
% Comprehensive references for long-context LLM research
% ============================================================================

% ----- Foundational Transformer Papers -----
@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  pages={5998--6008},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2019}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% ----- Position Encoding Papers -----
@inproceedings{press2022train,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Generalization},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{su2021roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{chen2023extending,
  title={Extending Context Window of Large Language Models via Positional Interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{ding2024longrope,
  title={LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@article{peng2024yarn,
  title={YaRN: Efficient Context Window Extension of Large Language Models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shao, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2024}
}

% ----- Streaming and Attention Sinks -----
@article{xiao2023streaming,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{han2024lminfinite,
  title={LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models},
  author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
  journal={arXiv preprint arXiv:2308.16137},
  year={2024}
}

% ----- State Space Models -----
@article{gu2024mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2024}
}

@article{dao2024mamba2,
  title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stellan and Cao, Huanqi and Cheng, Xin and Chung, Michael and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@article{yang2024gated,
  title={Gated Linear Attention Transformers with Hardware-Efficient Training},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2312.06635},
  year={2024}
}

% ----- KV Cache Optimization -----
@article{zhang2024h2o,
  title={H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hooper2024kvquant,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

% ----- Prompt Compression -----
@inproceedings{jiang2023llmlingua,
  title={LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models},
  author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={13358--13376},
  year={2023}
}

@article{jiang2024longllmlingua,
  title={LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression},
  author={Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.06839},
  year={2024}
}

@article{zhang2025scope,
  title={SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation},
  author={Zhang, Jialong and Chen, Zhiyuan and Zhang, Pengcheng and Shi, Baobao and Bian, Jiang},
  journal={arXiv preprint arXiv:2312.00936},
  year={2025}
}

@inproceedings{ge2024icae,
  title={In-context Autoencoder for Context Compression in a Large Language Model},
  author={Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{mu2024gisting,
  title={Learning to Compress Prompts with Gist Tokens},
  author={Mu, Jesse and Li, Xiang Lisa and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% ----- Retrieval-Augmented Generation -----
@inproceedings{lewis2020retrieval,
  title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{zhang2024gor,
  title={Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs},
  author={Zhang, Haozhen and Xu, Tao and Wang, Wentao and Bai, Yu and Wang, Bin and Zhao, Chen and Wang, Wenqiang and Chen, Kejun and Gao, Xing},
  journal={arXiv preprint arXiv:2410.11001},
  year={2024}
}

@article{zhang2025briefcontext,
  title={BriefContext: Efficient Long-Context Reasoning via Partitioning},
  author={Zhang, Chen and Liu, Yang and Zhou, Jiaying and Wang, Zhiyu and Jiang, Jing},
  journal={arXiv preprint arXiv:2401.00789},
  year={2025}
}

% ----- System Optimizations -----
@article{kwon2023vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E and Zhang, Hao and Stoica, Ion},
  journal={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dao2024flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2024}
}

@article{shah2024flashattention3,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}

@article{leviathan2023speculative,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  journal={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023}
}

% ----- Benchmarks and Evaluation -----
@article{rae2019compressive,
  title={Compressive Transformers for Long-Range Sequence Modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}

@article{hsieh2024ruler,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Krez, Samuel and Lyu, Fangzhao and Yu, Haining and Kim, Yejin and Krishna, Ranjay and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}

@article{kamradt2023needle,
  title={Needle in a Haystack - Pressure Testing LLMs},
  author={Kamradt, Greg},
  journal={GitHub repository},
  year={2023}
}

@article{liu2024lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024}
}

% ----- Advanced Architectures -----
@article{wang2024recat,
  title={ReCAT: Augmenting LLMs with Recursive Composition for Long-Context Understanding},
  author={Wang, Yash and Zhang, Lei and Li, Peng},
  journal={arXiv preprint arXiv:2309.16319},
  year={2024}
}

@article{sun2024gatedattention,
  title={Gated Attention: A Unified Framework for Eliminating Attention Sinks},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  year={2024}
}

@article{zhang2024nsa,
  title={Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention},
  author={Zhang, Jingyang and Chen, Yinghao and Peng, Yuxin and Zhang, Chi and Yang, Fan},
  journal={arXiv preprint arXiv:2502.11089},
  year={2024}
}

% ----- Memory and Context Management -----
@article{packer2023memgpt,
  title={MemGPT: Towards LLMs as Operating Systems},
  author={Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G and Stoica, Ion and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2310.08560},
  year={2023}
}

@article{yang2024rlm,
  title={Recursive Language Models: Towards Infinite Context via Self-Referential Compression},
  author={Yang, Zhengdong and Liu, Tianyu and Huang, Xuan and Li, Lei},
  journal={arXiv preprint arXiv:2501.09789},
  year={2024}
}

@article{supermemory2024,
  title={Supermemory: Semantic Compression for Infinite Chat Context},
  author={Supermemory Team},
  journal={Technical Report},
  year={2024}
}

% ----- Production Systems -----
@techreport{anthropic2024caching,
  title={Prompt Caching for Claude},
  author={Anthropic},
  institution={Anthropic},
  year={2024}
}

@techreport{openai2024caching,
  title={OpenAI Prompt Caching},
  author={OpenAI},
  institution={OpenAI},
  year={2024}
}

@article{google2026gemini,
  title={Gemini 3: 10 Million Token Context Window},
  author={Google DeepMind},
  journal={Technical Report},
  year={2026}
}

% ----- Linear Attention Theory -----
@inproceedings{choromanski2021rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{katharopoulos2020linear,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  journal={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020}
}

@article{peng2024pac,
  title={Polynomial-Time Learnability of Multi-Head Linear Attention},
  author={Peng, Haoyu and Zhang, Zhiyuan and Chen, Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  year={2024}
}

% ----- Quantization and Compression -----
@article{dettmers2024qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{frantar2023gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={International Conference on Learning Representations},
  year={2023}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={Machine Learning and Systems},
  year={2024}
}

% ----- Continual Pre-Training -----
@article{gupta2023continual,
  title={Continual Pre-Training of Language Models: A Comprehensive Survey},
  author={Gupta, Tanmay and Ramprasad, Revanth and Sahoo, Doyen and Hoi, Steven},
  journal={arXiv preprint arXiv:2302.03241},
  year={2023}
}

@article{ibrahim2024cpt,
  title={Simple and Scalable Strategies to Continually Pre-train Large Language Models},
  author={Ibrahim, Adam and Théoret, Benjamin and Panahi, Yared and Monnin, Éric and Wu, Qinqing and Baker, Bonaventure and Deleu, Tristan and Courville, Aaron and Larochelle, Hugo and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2403.08763},
  year={2024}
}

% ----- Mixture of Experts -----
@article{fedus2022switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022}
}

@article{jiang2024mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

% ----- Vector Quantization -----
@article{zhang2024vql,
  title={VQ-based One-to-Many Retrieval for Efficient Attention},
  author={Zhang, Yuxuan and Li, Qingkai and Chen, Hao},
  journal={arXiv preprint arXiv:2508.17125},
  year={2024}
}

% ----- LLaMA and Foundation Models -----
@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={LLaMA 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama3,
  title={The LLaMA 3 Herd of Models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% ----- Multi-Modal Long Context -----
@inproceedings{ye2024voco,
  title={VoCo-LLaMA: Towards Vision Compression with Large Language Models},
  author={Ye, Xubing and Wu, Yukang and Yang, Shuai and Pan, Zijian and Zhang, Zecheng and Wang, Wenxuan},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}

% ----- Recurrent Attention -----
@inproceedings{wang2023ran,
  title={Recurrent Attention Networks for Long-Range Modeling},
  author={Wang, Jiayun and Fan, Huimin and Xu, Kai and Xie, Jin},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={2954--2972},
  year={2023}
}

% ----- Adaptive Speculative Decoding -----
@article{chen2024adasd,
  title={Adaptive Speculative Decoding for Long-Context LLM Inference},
  author={Chen, Zhongying and Liu, Hao and Zhang, Chen and Li, Wei},
  journal={arXiv preprint arXiv:2512.11280},
  year={2024}
}

% ----- Context Extension Survey -----
@article{huang2024longcontext,
  title={Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey},
  author={Huang, Yunpeng and Xu, Jingwei and Jiang, Zixu and Lai, Junyu and Li, Zenan and Yao, Yuan and Chen, Taolue and Yang, Lijuan and Xin, Zhou and Ma, Xiaoxing},
  journal={arXiv preprint arXiv:2311.12351},
  year={2024}
}

@article{li2024prompt,
  title={Prompt Compression: A Comprehensive Survey},
  author={Li, Zongqian and Zhang, Yinhong and Huang, Jiajun and Chen, Bo},
  journal={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2024}
}
